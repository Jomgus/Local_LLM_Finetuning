Local Fine-Tuning of Phi-3 for Summarization

This project is an end to end prototype for fine-tuning the `microsoft/Phi-3-mini-4k-instruct` language model for a news article summarization task. The entire workflow, from model preparation to final evaluation, was performed locally on a 16GB M4 Macbook air using the MLX framework.

## Goal

The primary goal was to determine if it's feasible to measurably improve a modern LLM's performance on a specific task using a macbook. This was achieved using parameter-efficient fine-tuning (PEFT) with LoRA.

## Results

Results after fine-tuning show a measurable increase of .5 and 1.1% for the different ROGUE scores tested. These are results after training for just 100 iterations on a memory constrained system.


The model's performance was measured using the **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)** score. This metric works by comparing the summary generated by the model to a reference summary written by a human.
-   **ROUGE-1** measures the overlap of individual words.
-   **ROUGE-L** measures the overlap of the longest common sentence structure, which is often the key metric for summarization quality.

SCORES FROM 0 - 1

| Metric | Before (Baseline) | After (Fine-Tuned) | Improvement |
| :--- | :--- | :--- | :--- |
| **ROUGE-1** | 0.1818 | 0.1827 | +0.5% |
| **ROUGE-L** | **0.1343** | **0.1358** | **+1.1%** |

These results are modest, but I think the takeaway here is that the scores are moving in the right direction. Model is learning and improving at a rate we would expect for only 100 iterations. More ambitious fine-tuning is feasible on a base m4 macbook air.  

## Inspiration 

This project drew inspiration from several online resources while troubleshooting the fine-tuning process.

-   **[StrathWeb's "Fine tuning Phi models with MLX"](https://www.strathweb.com/2025/01/fine-tuning-phi-models-with-mlx/)**: This article served as a valuable roadmap. It demonstrated a successful end-to-end fine-tuning of the same Phi-3 model, showing the correct data formatting required by the `mlx-lm` training scripts.
-   **[heidloff.net's "Fine-tuning LLMs with Apple MLX locally"](https://heidloff.net/article/apple-mlx-fine-tuning/)**: This article was a sanity check, confirming that the mlx documentation was not reliable about how it wants the data formatted.

## Reproduce Results

Follow these steps to set up the environment and run through the full fine-tuning pipeline.

### 1. Environment Setup

First, set up the Python virtual environment and install the required libraries.

```

```bash
# Clone the repository
git clone https://github.com/Jomgus/Local_LLM_Finetuning
cd lora_project
```

```

# Create and activate a Python 3.11 virtual environment
python3.11 -m venv llm_fine_tuning_venv
source llm_fine_tuning_venv/bin/activate

# Install required packages
pip install mlx-lm datasets==2.16.1 evaluate rouge_score sentencepiece huggingface-hub

```

### 2. Model Preparation (Quantization)

Download the Phi-3 model from Hugging Face and convert it to a memory-efficient 4-bit quantized version.

```bash
python -m mlx_lm.convert \
    --hf-path microsoft/Phi-3-mini-4k-instruct \
    -q \
    --mlx-path phi3_quantized
```

### 3. Data Preparation

Prepare the training, validation, and test datasets from the `xsum` dataset.

```bash
python scripts/prepare_data.py
```

### 4. Run Baseline Benchmark

Test the base quantized model to get the "before" score.

```bash
python scripts/run_summarization_benchmark.py \
    --model-path ./phi3_quantized \
    --data-path ./summarization_data/test.jsonl \
    --output-file ./results/baseline_summarization_results.jsonl \
    --score-file ./results/baseline_benchmark_scores.txt
```

### 5. Fine-Tuning with LoRA

Run the fine-tuning process. This command includes memory-saving optimizations (`--grad-checkpoint`).

```bash
python -m mlx_lm.lora \
    --model ./phi3_quantized \
    --train \
    --data ./summarization_data/ \
    --iters 100 \
    --num-layers 8 \
    --grad-checkpoint
```

### 6. Fuse the Model

Merge the trained adapter with the base model to create the final, fine-tuned model.

```bash
python -m mlx_lm.fuse \
    --model ./phi3_quantized \
    --adapter-path ./adapters \
    --save-path ./lora_fused_model
```

### 7. Run Final Benchmark

Finally, test the new fused model to get the "after" score and see if improvement.

```bash
# First, prepare the specific test file for the benchmark
python scripts/prepare_benchmark_data.py

# Then, run the benchmark on the new model
python scripts/run_summarization_benchmark.py \
    --model-path ./lora_fused_model \
    --data-path ./benchmark_test.jsonl \
    --output-file ./results/final_summarization_results.jsonl \
    --score-file ./results/final_benchmark_scores.txt
```

# File Structure

This github is composed of two main parts:

A scripts folder: Contains the three essential scripts for preparing the data and running the benchmarks.

A results folder: Holds the .txt and .jsonl files with the final scores and raw outputs from both the baseline and the fine-tuned model tests.
